{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入所需的库\n",
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# hugging face的分词器，github地址：https://github.com/huggingface/tokenizers\n",
    "from tokenizers import Tokenizer\n",
    "# 用于构建词典\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.nn.functional import pad, log_softmax\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工作目录、为Transformer放数据、缓存文件的目录\n",
    "work_dir = Path(\"./dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练好的模型会放在该目录下，注意隔一段时间就要对模型进行保存，这是深度学习训练的基本\n",
    "model_dir = Path(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 上次运行到的地方，如果是第一次运行，为None，如果中途暂停了，下次运行时，指定目前最新的模型即可。\n",
    "model_checkpoint = None # 'model_10000.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#如果工作目录不存在，则创建一个\n",
    "if not os.path.exists(work_dir):\n",
    "    os.makedirs(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果模型目录不存在，则创建一个\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文句子的文件路径/数据集路径，\n",
    "en_filepath = './dataset/train.en'\n",
    "# 中文句子的文件路径/数据集路径\n",
    "zh_filepath = './dataset/train.zh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentences (first 5 lines):\n",
      "A pair of red - crowned cranes have staked out their nesting territory\n",
      "A pair of crows had come to nest on our roof as if they had come for Lhamo.\n",
      "A couple of boys driving around in daddy's car.\n",
      "A pair of nines? You pushed in with a pair of nines?\n",
      "Fighting two against one is never ideal,\n"
     ]
    }
   ],
   "source": [
    "with open(en_filepath, 'r', encoding='utf-8') as file:\n",
    "    #读取所有行\n",
    "    english_sentences = file.readlines()\n",
    "    print(\"English sentences (first 5 lines):\")\n",
    "    for line in english_sentences[:5]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chinese sentences (first 5 lines):\n",
      "一对丹顶鹤正监视着它们的筑巢领地\n",
      "一对乌鸦飞到我们屋顶上的巢里，它们好像专门为拉木而来的。\n",
      "一对乖乖仔开着老爸的车子。\n",
      "一对九？一对九你就全下注了？\n",
      "一对二总不是好事，\n"
     ]
    }
   ],
   "source": [
    "with open(zh_filepath, 'r', encoding='utf-8') as file:\n",
    "    #读取所有行\n",
    "    chinese_sentences = file.readlines()\n",
    "    print(\"\\nChinese sentences (first 5 lines):\")\n",
    "    for line in chinese_sentences[:5]:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个获取文件行数的办法，在开始训练之前，一定要保证离开两种语言对应的句子是一一对应的\n",
    "def get_row_count(filepath):\n",
    "    count = 0\n",
    "    for _ in open(filepath, encoding='utf-8'):\n",
    "        count += 1\n",
    "    return count\n",
    "en_row_count = get_row_count(en_filepath)\n",
    "zh_row_count  = get_row_count(zh_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zh_row_count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_row_count == zh_row_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句子数量，主要用于后面显示进度。\n",
    "row_count = en_row_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assert  en_row_count != zh_row_count #Assert是一种让代码主动发起报错的经典Python用法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文句子 平均长度 = 54.21, 最大长度 = 1039, 方差 = 1717.04\n",
      "中文句子: 平均长度 = 17.17, 最大长度 = 519, 方差 = 162.83\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def sample_file_statistics(filepath, sample_fraction=0.01):\n",
    "    # 初始化统计变量\n",
    "    total_length = 0\n",
    "    length_square_sum = 0\n",
    "    max_length = 0\n",
    "    count = 0\n",
    "    \n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # 计算行长度\n",
    "            length = len(line.strip())\n",
    "            # 累加总长度\n",
    "            total_length += length\n",
    "            # 累加长度的平方，用于方差计算\n",
    "            length_square_sum += length ** 2\n",
    "            # 更新最大长度\n",
    "            max_length = max(max_length, length)\n",
    "            # 计数\n",
    "            count += 1\n",
    "\n",
    "    # 计算平均长度和方差\n",
    "    if count > 0:\n",
    "        average_length = total_length / count\n",
    "        variance = (length_square_sum - (total_length ** 2) / count) / count\n",
    "    else:\n",
    "        average_length = 0\n",
    "        variance = 0\n",
    "\n",
    "    return average_length, max_length, variance\n",
    "\n",
    "# 抽样并计算英文和中文句子的平均长度、最大长度和方差\n",
    "en_stats = sample_file_statistics(en_filepath)\n",
    "zh_stats = sample_file_statistics(zh_filepath)\n",
    "\n",
    "print(f\"英文句子 平均长度 = {en_stats[0]:.2f}, 最大长度 = {en_stats[1]}, 方差 = {en_stats[2]:.2f}\")\n",
    "print(f\"中文句子: 平均长度 = {zh_stats[0]:.2f}, 最大长度 = {zh_stats[1]}, 方差 = {zh_stats[2]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "句子数量为： 10000000\n",
      "句子最大长度为： 72\n"
     ]
    }
   ],
   "source": [
    "#定义句子最大长度，如果句子不够长，填充，如果超出裁剪\n",
    "max_length = 72 #方差比较大，设置长一点\n",
    "print(\"句子数量为：\", en_row_count)\n",
    "print(\"句子最大长度为：\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义中英文词典\n",
    "en_vocab = None\n",
    "zh_vocab = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义bathc_size, 由于是文本，占用内存小，可以大一点\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epochs 不用设置太大，句子越多，需要的训练次数影响越小\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_after_step = 5000 #多少小步保存一次模型，防止程序崩溃导致模型丢失\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#是否使用缓存,由于文件较大，初始化松佐较慢，所以初始化好的文件需要持久化\n",
    "use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义训练设备\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size: 128\n",
      "每5000步保存一次模型\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"batch_size:\", batch_size)\n",
    "print(\"每{}步保存一次模型\".format(save_after_step))\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!ping huggingface.co "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载基础分词器模型，使用的是基础的bert,`uncased`意思是不区分大小写\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def en_tokenizer(line):\n",
    "#     \"\"\"\n",
    "#     定义英文分词器，后续也要使用\n",
    "#     :param line: 一句英文句子，例如\"I'm learning Deep learning.\"\n",
    "#     :return: subword分词后的记过，例如：['i', \"'\", 'm', 'learning', 'deep', 'learning', '.']\n",
    "#     \"\"\"\n",
    "#     # 使用bert进行分词，并获取tokens。add_special_tokens是指不要在结果中增加‘<bos>’和`<eos>`等特殊字符\n",
    "#     return tokenizer.encode(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def en_tokenizer(line):\n",
    "    return tokenizer.convert_ids_to_tokens(tokenizer.encode(line,  add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', \"'\", 'm', 'a', 'english', 'token', '##izer', '.']\n"
     ]
    }
   ],
   "source": [
    "print(en_tokenizer(\"I'm a English tokenizer.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_en_tokens():\n",
    "    \"\"\"\n",
    "    每次yield一个分词之后的英文句子，之所以yield是为了节省内存\n",
    "    如果先分好词语再构建词典，会有大量文本驻留内存，造成内存溢出\n",
    "    \"\"\"\n",
    "    flie = open(en_filepath, encoding='utf-8')\n",
    "    for line in tqdm(flie,desc=\"构建英文词典\", total=row_count):\n",
    "        yield en_tokenizer(line)\n",
    "    flie.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#指定英文词典缓存路径\n",
    "en_vocab_file = work_dir / \"vocab_en.pt\"\n",
    "#如果使用缓存，且文件存在，加载缓存文件\n",
    "if use_cache and os.path.exists(en_vocab_file):\n",
    "    en_vocab = torch.load(en_vocab_file, map_location=\"cpu\")\n",
    "else:\n",
    "    en_vocab = build_vocab_from_iterator(\n",
    "        # 传入一个形如[['i', 'am', ...], ['machine', 'learning', ...], ...]的可迭代列表\n",
    "        #这里可以理解直接传入了一个迭代器\n",
    "        yield_en_tokens(),\n",
    "        #最小频率为2，即一个单词最少收录两次才会被收录入到词典\n",
    "        min_freq = 2,\n",
    "        #在词典最开始加上这些特殊token\n",
    "        specials = [\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"],\n",
    "    )\n",
    "    #设置词典默认的index,后面文本转index时，如果找不到,就会用这个index\n",
    "    en_vocab.set_default_index(en_vocab[\"<unk>\"])\n",
    "    #保存缓存文件\n",
    "    if use_cache:\n",
    "        torch.save(en_vocab, en_vocab_file)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文词典大小： 27584\n",
      "{0: '<s>', 1: '</s>', 2: '<pad>', 3: '<unk>', 4: '.', 5: ',', 6: 'the', 7: \"'\", 8: 'i', 9: 'you'}\n"
     ]
    }
   ],
   "source": [
    "#查查查看一下词典的前10个词\n",
    "print(\"英文词典大小：\", len(en_vocab))\n",
    "print(dict((i, en_vocab.lookup_token(i)) for i in range(10)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造中文词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zh_tokenizer(line):\n",
    "    \"\"\"\n",
    "    定义中文分词器\n",
    "    :param line: 中文句子，例如：机器学习\n",
    "    :return: 分词结果，例如['机','器','学','习']\n",
    "    \"\"\"\n",
    "    #strip 删除头尾指定字符，默认是空格\n",
    "    return list(line.strip().replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "def yield_zh_tokens():\n",
    "    file = open(zh_filepath, encoding='utf-8')\n",
    "    for line in tqdm(file, desc=\"构建中文词典\", total=row_count):\n",
    "        yield zh_tokenizer(line)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "zh_vocab_file = work_dir / \"vocab_zh.pt\"\n",
    "if use_cache and os.path.exists(zh_vocab_file):\n",
    "    zh_vocab = torch.load(zh_vocab_file, map_location=\"cpu\")\n",
    "else:\n",
    "    zh_vocab = build_vocab_from_iterator(\n",
    "        yield_zh_tokens(),\n",
    "        min_freq=1,\n",
    "        specials=[\"<s>\", \"</s>\", \"<pad>\", \"<unk>\"],\n",
    "    )\n",
    "    zh_vocab.set_default_index(zh_vocab[\"<unk>\"])\n",
    "    torch.save(zh_vocab, zh_vocab_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文词典大小: 8280\n",
      "{0: '<s>', 1: '</s>', 2: '<pad>', 3: '<unk>', 4: '。', 5: '的', 6: '，', 7: '我', 8: '你', 9: '是'}\n"
     ]
    }
   ],
   "source": [
    "# 打印看一下效果，用前10个词为例\n",
    "print(\"中文词典大小:\", len(zh_vocab))\n",
    "print(dict((i, zh_vocab.lookup_token(i)) for i in range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 40, 516, 281, 11, 9, 27, 64]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "zh_vocab( zh_tokenizer(\"一对二总不是好事\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据批量加载为tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "        # 加载英文tokens\n",
    "        self.en_tokens = self.load_tokens(en_filepath, en_tokenizer, en_vocab, \"构建英文tokens\", 'en')\n",
    "        # 加载中文tokens\n",
    "        self.zh_tokens = self.load_tokens(zh_filepath, zh_tokenizer, zh_vocab, \"构建中文tokens\", 'zh')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.en_tokens[index], self.zh_tokens[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return row_count\n",
    "\n",
    "    def load_tokens(self, file, tokenizer, vocab, desc, lang):\n",
    "        \"\"\"\n",
    "        加载tokens，即将文本句子们转换成index们。\n",
    "        :param file: 文件路径，例如\"./dataset/train.en\"\n",
    "        :param tokenizer: 分词器，例如en_tokenizer函数\n",
    "        :param vocab: 词典, Vocab类对象。例如 en_vocab\n",
    "        :param desc: 用于进度显示的描述，例如：构建英文tokens\n",
    "        :param lang: 语言。用于构造缓存文件时进行区分。例如：’en‘\n",
    "        :return: 返回构造好的tokens。例如：[[6, 8, 93, 12, ..], [62, 891, ...], ...]\n",
    "        \"\"\"\n",
    "\n",
    "        # 定义缓存文件存储路径\n",
    "        cache_file = work_dir / \"tokens_list.{}.pt\".format(lang)\n",
    "        # 如果使用缓存，且缓存文件存在，则直接加载\n",
    "        if use_cache and os.path.exists(cache_file):\n",
    "            print(f\"正在加载缓存文件{cache_file}, 请稍后...\")\n",
    "            return torch.load(cache_file, map_location=\"cpu\")\n",
    "\n",
    "        # 从0开始构建，定义tokens_list用于存储结果\n",
    "        tokens_list = []\n",
    "        # 打开文件\n",
    "        with open(file, encoding='utf-8') as file:\n",
    "            # 逐行读取\n",
    "            for line in tqdm(file, desc=desc, total=row_count):\n",
    "                # 进行分词\n",
    "                tokens = tokenizer(line)\n",
    "                # 将文本分词结果通过词典转成index\n",
    "                tokens = vocab(tokens)\n",
    "                # append到结果中\n",
    "                tokens_list.append(tokens)\n",
    "        # 保存缓存文件\n",
    "        if use_cache:\n",
    "            torch.save(tokens_list, cache_file)\n",
    "\n",
    "        return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载缓存文件dataset\\tokens_list.en.pt, 请稍后...\n",
      "正在加载缓存文件dataset\\tokens_list.zh.pt, 请稍后...\n"
     ]
    }
   ],
   "source": [
    "dataset = TranslationDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([11, 2730, 12, 554, 19, 17210, 18077, 27, 3078, 203, 57, 102, 18832, 3653], [12, 40, 1173, 1084, 3169, 164, 693, 397, 84, 100, 14, 5, 1218, 2397, 535, 67])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "填充与裁剪句子，使句子等长"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    将dataset的数据进一步处理，并组成一个batch。\n",
    "    :param batch: 一个batch的数据，例如：\n",
    "                  [([6, 8, 93, 12, ..], [62, 891, ...]),\n",
    "                  ....\n",
    "                  ...]\n",
    "    :return: 填充后的且等长的数据，包括src, tgt, tgt_y, n_tokens\n",
    "             其中src为原句子，即要被翻译的句子\n",
    "             tgt为目标句子：翻译后的句子，但不包含最后一个token\n",
    "             tgt_y为label：翻译后的句子，但不包含第一个token，即<bos>\n",
    "             n_tokens：tgt_y中的token数，<pad>不计算在内。\n",
    "    \"\"\"\n",
    "\n",
    "    # 定义'<bos>'的index，在词典中为0，所以这里也是0\n",
    "    bs_id = torch.tensor([0])\n",
    "    # 定义'<eos>'的index\n",
    "    eos_id = torch.tensor([1])\n",
    "    # 定义<pad>的index\n",
    "    pad_id = 2\n",
    "\n",
    "    # 用于存储处理后的src和tgt\n",
    "    src_list, tgt_list = [], []\n",
    "\n",
    "    # 循环遍历句子对儿\n",
    "    for (_src, _tgt) in batch:\n",
    "        \"\"\"\n",
    "        _src: 英语句子，例如：`I love you`对应的index\n",
    "        _tgt: 中文句子，例如：`我 爱 你`对应的index\n",
    "        \"\"\"\n",
    "\n",
    "        processed_src = torch.cat(\n",
    "            # 将<bos>，句子index和<eos>拼到一块\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    _src,\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "        processed_tgt = torch.cat(\n",
    "            [\n",
    "                bs_id,\n",
    "                torch.tensor(\n",
    "                    _tgt,\n",
    "                    dtype=torch.int64,\n",
    "                ),\n",
    "                eos_id,\n",
    "            ],\n",
    "            0,\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "        将长度不足的句子进行填充到max_padding的长度的，然后增添到list中\n",
    "\n",
    "        pad：假设processed_src为[0, 1136, 2468, 1349, 1]\n",
    "             第二个参数为: (0, 72-5)\n",
    "             第三个参数为：2\n",
    "        则pad的意思表示，给processed_src左边填充0个2，右边填充67个2。\n",
    "        最终结果为：[0, 1136, 2468, 1349, 1, 2, 2, 2, ..., 2]\n",
    "        \"\"\"\n",
    "        src_list.append(\n",
    "            pad(\n",
    "                processed_src,\n",
    "                (0, max_length - len(processed_src),),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "        tgt_list.append(\n",
    "            pad(\n",
    "                processed_tgt,\n",
    "                (0, max_length - len(processed_tgt),),\n",
    "                value=pad_id,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # 将多个src句子堆叠到一起\n",
    "    src = torch.stack(src_list)\n",
    "    tgt = torch.stack(tgt_list)\n",
    "\n",
    "    # tgt_y是目标句子去掉第一个token，即去掉<bos>\n",
    "    tgt_y = tgt[:, 1:]\n",
    "    # tgt是目标句子去掉最后一个token\n",
    "    tgt = tgt[:, :-1]\n",
    "\n",
    "    # 计算本次batch要预测的token数\n",
    "    n_tokens = (tgt_y != 2).sum()\n",
    "\n",
    "    # 返回batch后的结果\n",
    "    return src, tgt, tgt_y, n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.tensor([0]),torch.tensor([0])], 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataLoader的本质是从dataset中取出一个batch的数据后，将一整个batch的数据放入函数collate_fn中处理后返回\n",
    "#默认的collate_fn函数是\n",
    "'''\n",
    "collate_fn=lambda x:(\n",
    " torch.cat(\n",
    "  [x[i][j].unsqueeze(0) for i in range(len(x))], 0\n",
    "  ) for j in range(len(x[0]))\n",
    " )\n",
    "'''\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "src, tgt, tgt_y, n_tokens = next(iter(train_loader))\n",
    "src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src.size: torch.Size([128, 72])\n",
      "tgt.size: torch.Size([128, 71])\n",
      "tgt_y.size: torch.Size([128, 71])\n",
      "n_tokens: tensor(2410)\n"
     ]
    }
   ],
   "source": [
    "print(\"src.size:\", src.size())\n",
    "print(\"tgt.size:\", tgt.size())\n",
    "print(\"tgt_y.size:\", tgt_y.size())\n",
    "print(\"n_tokens:\", n_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"进行位置编码.\"\n",
    "\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # 初始化Shape为(max_len, d_model)的PE (positional encoding)\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        # 初始化一个tensor [[0, 1, 2, 3, ...]]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        # 这里就是sin和cos括号中的内容，通过e和ln进行了变换\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        # 计算PE(pos, 2i)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 计算PE(pos, 2i+1)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 为了方便计算，在最外面在unsqueeze出一个batch\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 如果一个参数不参与梯度下降，但又希望保存model的时候将其保存下来\n",
    "        # 这个时候就可以用register_buffer\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x 为embedding后的inputs，例如(1,7, 128)，batch size为1,7个单词，单词维度为128\n",
    "        \"\"\"\n",
    "        # 将x和positional encoding相加。\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding与掩码以及模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, src_vocab, tgt_vocab, dropout=0.1):\n",
    "        super(TranslationModel, self).__init__()\n",
    "\n",
    "        # 定义原句子的embedding\n",
    "        # embedding的维度被设置成了超参数\n",
    "        # 在本次案例中我们使用的是256\n",
    "        self.src_embedding = nn.Embedding(len(src_vocab), d_model, padding_idx=2)\n",
    "        # 定义目标句子的embedding\n",
    "        self.tgt_embedding = nn.Embedding(len(tgt_vocab), d_model, padding_idx=2)\n",
    "        # 定义posintional encoding\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_len=max_length)\n",
    "        # 定义Transformer\n",
    "        self.transformer = nn.Transformer(d_model, dropout=dropout, batch_first=True)\n",
    "\n",
    "        # 定义最后的预测层，这里并没有定义Softmax，而是把他放在了模型外。\n",
    "        self.predictor = nn.Linear(d_model, len(tgt_vocab))\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        进行前向传递，输出为Decoder的输出。注意，这里并没有使用self.predictor进行预测，\n",
    "        因为训练和推理行为不太一样，所以放在了模型外面。\n",
    "        :param src: 原batch后的句子，例如[[0, 12, 34, .., 1, 2, 2, ...], ...]\n",
    "        :param tgt: 目标batch后的句子，例如[[0, 74, 56, .., 1, 2, 2, ...], ...]\n",
    "        :return: Transformer的输出，或者说是TransformerDecoder的输出。\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        生成tgt_mask，即阶梯型的mask，例如：\n",
    "        [[0., -inf, -inf, -inf, -inf],\n",
    "        [0., 0., -inf, -inf, -inf],\n",
    "        [0., 0., 0., -inf, -inf],\n",
    "        [0., 0., 0., 0., -inf],\n",
    "        [0., 0., 0., 0., 0.]]\n",
    "        tgt.size()[-1]为目标句子的长度。\n",
    "        \"\"\"\n",
    "        # 对目标句子，要掩盖住embedding矩阵的上半部分（代表从过去向未来询问的部分）\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size()[-1]).to(device)\n",
    "        # 但除此之外，我们还需要对所有填充的部分进行掩码，减少对模型的噪音干扰\n",
    "        # 掩盖住原句子中<pad>的部分，例如[[False,False,False,..., True,True,...], ...]\n",
    "        src_key_padding_mask = TranslationModel.get_key_padding_mask(src)\n",
    "        # 掩盖住目标句子中<pad>的部分\n",
    "        tgt_key_padding_mask = TranslationModel.get_key_padding_mask(tgt)\n",
    "\n",
    "        # 对src和tgt进行编码\n",
    "        src = self.src_embedding(src)\n",
    "        tgt = self.tgt_embedding(tgt)\n",
    "        # 给src和tgt的token增加位置信息\n",
    "        src = self.positional_encoding(src)\n",
    "        tgt = self.positional_encoding(tgt)\n",
    "        # 经过这一步之后，数据的结构变成了三维\n",
    "        #(batch_size, sentence_len, embedding_dimension)\n",
    "\n",
    "        # 将准备好的数据送给nn.transformer\n",
    "        # 在这里我们是一并将源数据与目标数据给到了transformer模型\n",
    "        # 但transformer模型实际上是先试用src原始数据\n",
    "        # 再试用tgt目标数据的\n",
    "        # 同时两个数据的掩码也各不相同\n",
    "        out = self.transformer(src, tgt,\n",
    "                               tgt_mask=tgt_mask,\n",
    "                               src_key_padding_mask=src_key_padding_mask,\n",
    "                               tgt_key_padding_mask=tgt_key_padding_mask)\n",
    "\n",
    "        \"\"\"\n",
    "        这里直接返回transformer的结果。因为训练和推理时的行为不一样，\n",
    "        所以在该模型外再进行线性层的预测。\n",
    "        \"\"\"\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def get_key_padding_mask(tokens):\n",
    "        \"\"\"\n",
    "        用于key_padding_mask\n",
    "        \"\"\"\n",
    "        return tokens == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint:\n",
    "    model = torch.load(model_dir / model_checkpoint)\n",
    "else:\n",
    "    model = TranslationModel(256, en_vocab, zh_vocab)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15913\\anaconda3\\envs\\pytorchcpu\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 71, 256])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(src, tgt).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.8971,  3.4824, -0.6170,  ...,  1.1734,  0.2916,  0.2956],\n",
      "        [-1.1413, -1.2990,  1.1521,  ...,  0.8747,  0.4199, -1.3246],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [-0.0337, -0.4230, -1.1757,  ..., -0.5287, -0.0297, -0.8514],\n",
      "        [ 0.9629, -0.6134, -1.0458,  ...,  1.0507,  0.2567, -0.5436],\n",
      "        [ 0.9033,  1.7036,  0.5967,  ..., -0.3990, -1.4044,  0.6190]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1],\n",
       "        [4, 3]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[1,2],[3,4]])\n",
    "torch.gather(t, 1, torch.tensor([[0,0],[1,0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationLoss(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(TranslationLoss, self).__init__()\n",
    "        # 使用KLDivLoss，不需要知道里面的具体细节。\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = 2\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        损失函数的前向传递\n",
    "        :param x: 将Decoder的输出再经过predictor线性层之后的输出。\n",
    "                  也就是Linear后、Softmax前的状态\n",
    "        :param target: tgt_y。也就是label，例如[[1, 34, 15, ...], ...]\n",
    "        :return: loss\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\"\n",
    "        由于KLDivLoss的input需要对softmax做log，所以使用log_softmax。\n",
    "        等价于：log(softmax(x))\n",
    "        \"\"\"\n",
    "        x = log_softmax(x, dim=-1)\n",
    "\n",
    "        \"\"\"\n",
    "        构造Label的分布，也就是将[[1, 34, 15, ...]] 转化为:\n",
    "        [[[0, 1, 0, ..., 0],\n",
    "          [0, ..., 1, ..,0],\n",
    "          ...]],\n",
    "        ...]\n",
    "        \"\"\"\n",
    "        # 首先按照x的Shape构造出一个全是0的Tensor\n",
    "        true_dist = torch.zeros(x.size()).to(device)\n",
    "        # 将对应index的部分填充为1\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), 1)\n",
    "        # 找出<pad>部分，对于<pad>标签，全部填充为0，没有1，避免其参与损失计算。\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "\n",
    "        # 计算损失\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria = TranslationLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(log_dir='runs/transformer_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step = 0\n",
    "\n",
    "# if model_checkpoint:\n",
    "#     step = int('model_10000.pt'.replace(\"model_\", \"\").replace(\".pt\", \"\"))\n",
    "\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     loop = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "#     for index, data in enumerate(train_loader):\n",
    "#         # 生成数据\n",
    "#         src, tgt, tgt_y, n_tokens = data\n",
    "#         src, tgt, tgt_y = src.to(device), tgt.to(device), tgt_y.to(device)\n",
    "\n",
    "#         # 清空梯度\n",
    "#         optimizer.zero_grad()\n",
    "#         # 进行transformer的计算\n",
    "#         out = model(src, tgt)\n",
    "#         # 将结果送给最后的线性层进行预测\n",
    "#         out = model.predictor(out)\n",
    "\n",
    "#         \"\"\"\n",
    "#         计算损失。由于训练时我们的是对所有的输出都进行预测，所以需要对out进行reshape一下。\n",
    "#                 我们的out的Shape为(batch_size, 词数, 词典大小)，view之后变为：\n",
    "#                 (batch_size*词数, 词典大小)。\n",
    "#                 而在这些预测结果中，我们只需要对非<pad>部分进行，所以需要进行正则化。也就是\n",
    "#                 除以n_tokens。\n",
    "#         \"\"\"\n",
    "#         loss = criteria(out.contiguous().view(-1, out.size(-1)), tgt_y.contiguous().view(-1)) / n_tokens\n",
    "#         # 计算梯度\n",
    "#         loss.backward()\n",
    "#         # 更新参数\n",
    "#         optimizer.step()\n",
    "\n",
    "#         loop.set_description(\"Epoch {}/{}\".format(epoch, epochs))\n",
    "#         loop.set_postfix(loss=loss.item())\n",
    "#         loop.update(1)\n",
    "\n",
    "#         step += 1\n",
    "\n",
    "#         del src\n",
    "#         del tgt\n",
    "#         del tgt_y\n",
    "\n",
    "#         if step != 0 and step % save_after_step == 0:\n",
    "#             torch.save(model, model_dir / f\"model_{step}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(r\"./model./model_75000.pt\")\n",
    "model = torch.load(path,   map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#设置为测试模式\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(src: str):\n",
    "    \"\"\"\n",
    "    :param src: 英文句子，例如 \"I like machine learning.\"\n",
    "    :return: 翻译后的句子，例如：”我喜欢机器学习“\n",
    "    \"\"\"\n",
    "\n",
    "    # 将与原句子分词后，通过词典转为index，然后增加<bos>和<eos>\n",
    "    src = torch.tensor([0] + en_vocab(en_tokenizer(src)) + [1]).unsqueeze(0).to(device)\n",
    "    # 首次tgt为<bos>\n",
    "    tgt = torch.tensor([[0]]).to(device)\n",
    "    # 一个一个词预测，直到预测为<eos>，或者达到句子最大长度\n",
    "    for i in range(max_length):\n",
    "        # 进行transformer计算\n",
    "        out = model(src, tgt)\n",
    "        # 预测结果，因为只需要看最后一个词，所以取`out[:, -1]`\n",
    "        predict = model.predictor(out[:, -1])\n",
    "        # 找出最大值的index\n",
    "        y = torch.argmax(predict, dim=1)\n",
    "        # 和之前的预测结果拼接到一起\n",
    "        tgt = torch.concat([tgt, y.unsqueeze(0)], dim=1)\n",
    "        # 如果为<eos>，说明预测结束，跳出循环\n",
    "        if y == 1:\n",
    "            break\n",
    "    # 将预测tokens拼起来\n",
    "    tgt = ''.join(zh_vocab.lookup_tokens(tgt.squeeze().tolist())).replace(\"<s>\", \"\").replace(\"</s>\", \"\")\n",
    "    return tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15913\\anaconda3\\envs\\pytorchcpu\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我能说什什什？我能说什什么？祝你一天好好日祝你一天气祝你一天气祝你一天好好美好的一天！祝你一天！祝你一天！祝你一天！美美美美美美美美美美美美美美'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"What can I say? Wish you a nice day!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'好，这个项完成了。我们看看这个项目有多好。'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#英译中测试\n",
    "translate(\"Alright, this project is finished. Let's see how good this is.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'很长长长长长。'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "translate(\"Long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'我喜喜喜喜喜喜喜喜喜喜喜喜喜喜喜机机机器学习机器学习机器学习。学习机器学习。'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "translate(\"I like machine learning.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'你怎你怎怎你怎怎怎怎你怎怎怎怎你怎样如怎你怎怎怎样？你怎怎你怎样如怎？你怎怎？你怎怎怎？你怎怎？你怎？你怎？你怎？你怎怎？你怎？你怎？你怎怎？你'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"How are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\15913\\anaconda3\\envs\\pytorchcpu\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我有只猫猫猫猫猫猫猫猫我有猫猫猫猫猫。'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(\"i have a cat.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchcpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
